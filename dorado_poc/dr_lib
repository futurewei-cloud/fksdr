# ==============================================================================================
#         Update dr.cfg to match test set up

. dr.cfg 

pause()
{
echo
echo -n "Hit ENTER when you are ready to $@: "
read ANSWER
}

sanity_check_file_exist()
{
   if [ ! -f "$1" ];then
      echo "ERROR: cannot locate file $1"
      exit
   fi
}

sanity_check_cfg()
{
sanity_check_file_exist ../bin/velero

if [ ! -f dr.cfg ];then
   echo "FATAL: can not locate DR configration file dr.cfg"
   exit
fi

echo
echo "Please confirm the DR configuration in dr.cfg:"
echo
cat dr.cfg

pause "proceed, or CTL-C to quit"
}

# ==============================================================================================

# High level steps:
# 1. Update app data, establish replication, wait for data to be mirrored
# 2. Split mirror and mount velero PVC
# 3. Reload velero on peer and load backup data
# 4. Restore
#

if [ ! -f .uuid ];then
    uuidgen > .uuid
fi
BACKUPID=$(cat .uuid)

TMPDIR=${BASE}/tmp/${BACKUPID}
PVF=${BASE}/pv-dorado3.yaml
PVCF=${BASE}/pvc-velero.yaml
POD=nginx-dorado1-fs1
BACKUP=vbackup-${BACKUPID}

RED='\033[0;31m'
GREEN='\033[0;32m'
NC='\033[0m' # No Color
pgreen()
{
   printf "\n${GREEN}$@${NC}\n"
}

pred()
{
   printf "\n${RED}$@${NC}\n"
}

sanity_check()
{
   kubectl get pv
   kubectl get pvc -A
   kubectl get pod -n velero
}

enter_velero()
{
   kubectl -n velero exec -ti `kubectl -n velero get pod | grep "^velero" | awk '{print $1}'` -- /bin/bash $@
}

drun_dev()
{
   DORADO=$1
   shift
   CMD="$@"
   sshpass -p Storage@21st ssh ibc_os_hs@${DORADO} "$CMD" 2>/dev/null
}

drun()
{
   DORADO=$1
   shift
   CMD="$@"
   sshpass -p Admin@storage6 ssh admin@${DORADO} "$CMD" 2>/dev/null
}

rest_cleanup()
{
   rm -f .*.rest .*.session
}

_rest_login()
{
   IP=$1
   curl -v --insecure -d '{"username":"admin","password":"Admin@storage6","scope":0}' \
      https://${IP}:8088/deviceManager/rest/xx/sessions 2>&1 | grep -e Set-Cookie -e iBaseToken > .${IP}.session
   SESSION=$(grep Set-Cookie .${IP}.session | tail -1 | sed "s/^.*Set-Cookie: \([^;]*\);.*$/\1/g")
   TOKEN=$(grep iBaseToken .${IP}.session | sed "s/^.*iBaseToken\":\"\([^\"]*\)\".*$/\1/g")
   DEVID=$(grep deviceid .${IP}.session | sed "s/^.*deviceid\":\"\([^\"]*\)\".*$/\1/g")
   echo "curl --insecure -b \"${SESSION}\" -H \"iBaseToken=${TOKEN}\" https://${IP}:8088/deviceManager/rest/${DEVID}/\$@" > .${IP}.rest
   chmod a+x .${IP}.rest
}

rest_login()
{
   [ $# -eq 0 ] && return
   IP=$1
   if [ ! -f .${IP}.session ];then
      _rest_login ${IP}
   else 
      if [ ! -z `find .${IP}.session -mmin +20` ];then
         echo refreshing expired session...
         _rest_login ${IP}
      fi
   fi
}

rest_exec()
{
   [ $# -lt 2 ] && return
   IP=$1;shift
   rest_login $IP
   #./.${IP}.rest $@ 2>/dev/null | jq .
   ./.${IP}.rest $@ 2>/dev/null | jq . | tee /tmp/.rest_response.${BACKUPID}
   if [ "`cat /tmp/.rest_response.${BACKUPID} | jq .error.code`" = "-401" ];then
      rm -f .${IP}*
   fi
   rm -f /tmp/.rest_response.${BACKUPID}
}

get_rep_ids()
{
   rest_exec ${DORADO_A} REPLICATIONPAIR | grep -e \"ID\" -e  LOCALRESNAME > .all.reps
   
   rm -f .reps
   kubectl get pv | tail -n +2 | awk '{print $1}' | while read PV;do
      LUN=$(kubectl describe pv $PV | grep VolumeHandle | sed "s/^.*pvc-/pvc-/g;s/ *//g")
      if [ `grep -c $LUN .all.reps` -gt 0 ];then
         REP=$(grep $LUN .all.reps -B 1 | head -1 | awk -F\" '{print $4}')
         echo $REP >> .reps
      fi
   done
}

get_stg_ips()
{
   rest_exec $DORADO_A eth_port | grep -e '"NAME"' -e "IPV4ADDR\": \"[1-9]" | sed '$!N;/\n.*MGMT/!P;D' | sed '$!N;/\n.*MAINTENANCE/!P;D' | grep IPV4ADDR | awk -F\" '{print $4}' > .ports.a
   rest_exec $DORADO_B eth_port | grep -e '"NAME"' -e "IPV4ADDR\": \"[1-9]" | sed '$!N;/\n.*MGMT/!P;D' | sed '$!N;/\n.*MAINTENANCE/!P;D' | grep IPV4ADDR | awk -F\" '{print $4}' > .ports.b
}

generate_pv()
{
kubectl get pv `kubectl get pv | grep velero/ | awk '{print $1}'` -o yaml > ${PVF}.tmp
cat ${PVF}.tmp | grep -i -v -e time -e "^  *uid" -e "^  *resourceVersion:" | sed -n '/^status/q;p' > $PVF
sed -i "s/operation:.*/operation: Apply/g" $PVF
}

start_rep()
{
echo > .start_rep
chmod +x .start_rep
while read REP;do
   #TODO: these REST commands will fail due to readonly access
   #rest_exec ${DORADO_A} REPLICATIONPAIR/$REP -d '{"SECRESACCESS":"2"}' 
   #rest_exec ${DORADO_A} REPLICATIONPAIR/sync -d "{\"ID\":\"${REP}\"}"

   #TODO: change to rest later
   echo "sshpass -p Admin@storage6 ssh admin@${DORADO_A} change remote_replication general remote_replication_id=${REP} second_res_access=read_only 2>/dev/null" >> .start_rep
   echo "sshpass -p Admin@storage6 ssh admin@${DORADO_A} change remote_replication synchronize remote_replication_id=${REP} 2>/dev/null" >> .start_rep
done<.reps
./.start_rep
}

stop_rep()
{
echo > .stop_rep
chmod +x .stop_rep
while read REP;do
   #TODO: these REST commands will fail due to readonly access
   #rest_exec ${DORADO_A} REPLICATIONPAIR/$REP -d '{"SECRESACCESS":"3"}' 
   #rest_exec ${DORADO_A} REPLICATIONPAIR/split -d "{\"ID\":\"${REP}\"}"

   #TODO: remove these SSH commands after REST is fixed
   echo "sshpass -p Admin@storage6 ssh admin@${DORADO_A} change remote_replication split remote_replication_id=${REP} 2>/dev/null" >> .stop_rep
   echo "sshpass -p Admin@storage6 ssh admin@${DORADO_A} change remote_replication general remote_replication_id=${REP} second_res_access=read_write 2>/dev/null" >> .stop_rep
done<.reps
./.stop_rep
}

# Inject some new data into primary site POD
update_primary()
{
   # Inject data into nginx POD
   pgreen "updating application data ..."
   echo "<html>" > /tmp/index.html
   echo "<h1>System log:</h1><BR><BR><pre>" >> /tmp/index.html
   tail -20 /var/log/syslog >> /tmp/index.html
   echo "</pre></html>" >> /tmp/index.html
   kubectl -n vtest cp /tmp/index.html $POD:/usr/share/nginx/html/
   kubectl -n vtest exec -ti $POD -- /bin/bash -c 'sync'
   sleep 3

   # Start a backup
   pred "starting application backup ..."
   kubectl annotate pv "$(kubectl -n vtest get pvc | awk '$3 ~/pvc/ { print $3 }')" tony.io/dr-protected-pv="Huawei-DR-Protected" --overwrite
   ../bin/velero backup create $BACKUP --include-namespaces vtest --storage-location tony-storagelocation --volume-snapshot-locations tony-snapshotlocation
   kubectl -n velero exec -ti `kubectl -n velero get pod | grep "^velero" | awk '{print $1}'` -- /bin/bash -c 'sync'
   pred "waiting backup data flush to storage ..."
   sleep 5
   kubectl -n velero exec -ti `kubectl -n velero get pod | grep "^velero" | awk '{print $1}'` -- /bin/bash -c 'sync'
   pgreen "backup data is ready now."
}

install_remote()
{
ssh $PEER `pwd`/install.sh
ssh $PEER `pwd`/../bin/velero plugin add ljtbbt/fksdr-plugin:v2
ssh $PEER `pwd`/../bin/velero backup-location create tony-storagelocation --provider example.io/object-store-plugin --bucket velero-pvc --credential cloud-credentials=cloud
ssh $PEER `pwd`/../bin/velero snapshot-location create tony-snapshotlocation --provider example.io/volume-snapshotter-plugin
ssh $PEER kubectl create namespace vtest
ssh $PEER kubectl -n velero patch deploy velero --patch "$(cat velero.deploy.json)"
}

prepare_remote()
{
ssh $PEER kubectl -n velero rollout restart deployment velero
}

setup_remote()
{
# =================== recreate PVC on peer ===================
ssh $PEER kubectl apply -f $PVF
ssh $PEER kubectl apply -f $PVCF
ssh $PEER 'kubectl -n velero patch deploy velero --patch "$(cat velero.deploy.json)"'
}

restore()
{
# =================== wait for velero POD to be ready ===================
echo
pred "waiting for remote site service pod to be ready ..."
while true;do
   READY=$(ssh $PEER kubectl -n velero get pod | grep -c "^velero.*Running")
   if [ $READY -eq 1 ];then
      break
   else
      echo -n .
      sleep 1
   fi
done

pgreen "remote site service pod is now ready."
echo
pred "waiting for backup data loading ..."

while true;do
   READY=$(ssh $PEER `pwd`/../bin/velero backup get | grep -c $BACKUP)
   if [ $READY -eq 1 ];then
      break
   else
      echo -n .
      sleep 1
   fi
done
pgreen "backup data is now available."

pause "start DR restore"
#echo
#echo -n "Hit ENTER when you are ready to start DR restore: "
#read ANSWER
echo "starting restore ..."
ssh $PEER `pwd`/../bin/velero restore create --from-backup $BACKUP

while true;do
   READY=$(ssh $PEER kubectl -n vtest get pod | grep -c "^nginx.*Running")
   if [ $READY -eq 1 ];then
      break
   else
      echo -n .
      sleep 1
   fi
done
pgreen "application pod is now ready."
}

